{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to _pyspark_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark and its related packages are constantly changing, and even the most basic scripts may become unusable from version to version. Threfore it is a good idea to be familiar with the documentation part, which is (trying to be) updated and helpful. Here are some relevant documentation links:\n",
    "\n",
    "* [Spark 2.0.2][spark]\n",
    "    * General concepts\n",
    "        * [Programing guide][pg]\n",
    "        * [Data structures][ds] - this includes explanations about DataFrames, DataSets and SQL\n",
    "    * Python API\n",
    "        * [pyspark package][pyspark] - this includes the [SparkConf][conf], [SparkContext][sc] and [RDD][rdd] classes\n",
    "        * [pyspark.sql module][sql] - this includes the [SparkSession][ss], [DataFrame][df], [Row][row] and [Column][col] classes\n",
    "\n",
    "[spark]: https://spark.apache.org/docs/2.0.2/index.html \"Spark 2.0.2\"\n",
    "[pg]: https://spark.apache.org/docs/2.0.2/programming-guide.html \"Spark programming guide\"\n",
    "[ds]: https://spark.apache.org/docs/2.0.2/sql-programming-guide.html \"Data structures programming guide\"\n",
    "[pyspark]: https://spark.apache.org/docs/2.0.2/api/python/index.html \"pyspark\"\n",
    "[conf]: https://spark.apache.org/docs/2.0.2/api/python/pyspark.html#pyspark.SparkConf \"SparkConf\"\n",
    "[sc]: https://spark.apache.org/docs/2.0.2/api/python/pyspark.html#pyspark.SparkContext \"SparkContext\"\n",
    "[rdd]: https://spark.apache.org/docs/2.0.2/api/python/pyspark.html#pyspark.RDD \"RDD\"\n",
    "[sql]: https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html \"pyspark.sql module\"\n",
    "[ss]: https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.SparkSession \"SparkSession\"\n",
    "[df]: https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame \"DataFrame\"\n",
    "[row]: https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.Row \"Row\"\n",
    "[col]: https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.Column \"Column\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1 - RDD fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the Moby Dick text and answer the questions below. \n",
    "\n",
    "> **NOTE:** See [here][dbfs] for details about uploading files to the DBFS.\n",
    "\n",
    "[dbfs]: https://docs.databricks.com/user-guide/importing-data.html \"DBFS uploading\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = sc\\\n",
    "    .textFile(\"/FileStore/tables/yg6aazpx1490278902208/melville_moby_dick-7006e.txt\")\\\n",
    "    .flatMap(lambda line: line.split())\\\n",
    "    .filter(lambda word: word.isalpha())\\\n",
    "    .map(lambda word: word.lower())\n",
    "words.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How may words are in the book? (words contain only letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many _unique_ words are in the book?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_words = words.groupBy(lambda word: word)\n",
    "unique_words.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_words.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the most common word in the book?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_count = unique_words\\\n",
    "    .mapValues(lambda group: len(group))\\\n",
    "    .sortBy(lambda word_count: word_count[1], ascending=False)\n",
    "print word_count.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the most common word in the book which is not a [stop-word][1]? (a file with the English stop-words is available in the folder)\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Stop_words \"Stop words - Wikipedia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = sc\\\n",
    "    .textFile(\"/FileStore/tables/pp6ku2g01490279937078/english_stop_words-12e81.txt\")\\\n",
    "    .map(lambda word: (word, 1))\n",
    "print stop_words.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_count_2 = word_count\\\n",
    "    .subtractByKey(stop_words)\\\n",
    "    .sortBy(lambda word_count: word_count[1], ascending=False)\n",
    "print word_count_2.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Your turn 1:** Read the file \"english words\" into an RDD and answer the following questions:\n",
    "> 1. How many words are listed in the file?\n",
    "> 2. What is the most common first letter?\n",
    "> 3. What is the longest word in the file?\n",
    "> 4. How many words include all 5 vowels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2 - DataFrames fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each record in the \"dessert\" dataset describes a group visit at a restaurant. Read the data and answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dessert = spark.read.csv(\"/FileStore/tables/sdztx2671490282633198/dessert.csv\", \n",
    "                         header=True, inferSchema=True)\\\n",
    "  .drop('id')\\\n",
    "  .withColumnRenamed('day.of.week', 'weekday')\\\n",
    "  .withColumnRenamed('num.of.guests', 'num_of_guests')\\\n",
    "  .withColumnRenamed('dessert', 'purchase')\\\n",
    "  .withColumnRenamed('hour', 'shift')\n",
    "dessert.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dessert.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataframeReader object used above is sometimes confusing, so I show below how to first load the data as an RDD, and then modify it into a dataFrame. During this process we also remove the header using a combination of _zipWithIndex()_ and _filter()_ (taken from [here][1]). By looking at the file we see the \"schema\", which is used by the second _map()_.\n",
    "\n",
    "[1]: http://stackoverflow.com/a/31798247/3121900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dessert_rdd = sc\\\n",
    "    .textFile(\"/FileStore/tables/sdztx2671490282633198/dessert.csv\")\\\n",
    "    .map(lambda line: line.split(','))\\\n",
    "    .zipWithIndex()\\\n",
    "    .filter(lambda tup: tup[1] > 0)\\\n",
    "    .map(lambda tup: [tup[0][1],           # weekday\n",
    "                      int(tup[0][2]),      # num_of_guests\n",
    "                      tup[0][3],           # shift\n",
    "                      int(tup[0][4]),      # table\n",
    "                      tup[0][5]=='TRUE'])  # purchase\n",
    "\n",
    "columns = ['weekday', 'num_of_guests', 'shift', 'table', 'purchase']\n",
    "dessert = spark.createDataFrame(dessert_rdd,\n",
    "                                schema=columns)\n",
    "dessert.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many groups purchased a dessert?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col = dessert.purchase\n",
    "dessert.where(col).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many groups purchased a dessert on Mondays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col = (dessert.weekday == 'Monday') & (dessert.purchase)\n",
    "dessert.where(col).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many _visitors_ purchased a dessert?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dessert\\\n",
    "    .where(dessert.purchase)\\\n",
    "    .agg({'num_of_guests': 'sum', 'table': 'mean'})\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each weekday - how many groups purchased a dessert?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dessert\\\n",
    "    .where(dessert.purchase)\\\n",
    "    .groupBy('weekday')\\\n",
    "    .agg({'shift': 'count', 'num_of_guests': 'sum'})\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add to _dessert_ a new column called 'no purchase' with the negative of 'purchse'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dessert = dessert.withColumn('no_purchase', ~dessert.purchase)\n",
    "dessert.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pivot table showing how the purchases were influenced by the size of the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dessert.crosstab('num_of_guests', 'weekday').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Your turn 2:** Read the file \"weights\" into an Dataframe and answer the following questions:\n",
    "> 1. Create a new Dataframe with the data of the males only and call it _males_.\n",
    "> 2. How many males are in the table? What is the mean height and weight of the males?\n",
    "> 3. What is the height of the tallest female who is older than 40?\n",
    "> 4. Create a new Dataframe with two columns for the age and the average weight of the people in this age."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "name": "Introduction to PySpark",
  "notebookId": 2317684797154294
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
