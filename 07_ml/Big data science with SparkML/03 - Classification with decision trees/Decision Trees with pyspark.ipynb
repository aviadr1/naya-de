{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, IndexToString\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easier (in this pyspark version) to first load the data as an RDD, and then modify it into a dataFrame. During this process we also remove the header using a combination of _zipWithIndex()_ and _filter()_ (taken from [here][1]). By looking at the file we see the \"schema\", which is used by the second _map()_.\n",
    "\n",
    "[1]: http://stackoverflow.com/a/31798247/3121900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "digits = spark.read.csv(\"/FileStore/tables/kox6mzsb1490346252745/digits.csv\", \n",
    "                        header=True, inferSchema=True)\n",
    "digits.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data (for ML API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained earlier, _pyspark.ml_ models expect the features to be assembled together as a single **vector**. For that we have to use the _VectorAssembler_ **transformer** and create a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "va = VectorAssembler(inputCols=list('abcdefg'),\n",
    "                     outputCol='features')\n",
    "digits = va.transform(digits)\n",
    "digits.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained earlier, _pyspark.ml_ models cannot deal with non-numeric datatypes, therefore we have to encode such data as numbers. To that end we use the _StringIndexer_ **estimator** and **transformer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "si = StringIndexer(inputCol='digit',\n",
    "                   outputCol='digit (indexed)')\n",
    "digits = si.fit(digits).transform(digits)\n",
    "digits.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on in the process we would probably apply the classification model which will inevitably produce the encoded (indexed) labels, so a natural question is how to interpret these numbers. Luckilly, like any other estimator, after fitting the data, the _StringIndexerModel_ holds some metadata of the model itself. This information is part of the data, and can be accessed by the following syntax (adopted from [this Stack Overflow answer][1]).\n",
    "\n",
    "[1]: http://stackoverflow.com/a/33903867/3121900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = digits.schema.fields[-1].metadata['ml_attr']['vals']\n",
    "print labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in a more readable fashion..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print list(enumerate(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even more exciting, we will be able to use the _IndexToString_ method to \"return the wheel\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = digits.randomSplit([0.7, 0.3],\n",
    "                                 seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we instantiate the [_DecisionTreeClassifier_][1] **estimator**.\n",
    "\n",
    "[1]: http://spark.apache.org/docs/2.0.2/api/python/pyspark.ml.html#pyspark.ml.classification.DecisionTreeClassifier \"DecisionTreeClassifier API\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(featuresCol='features',\n",
    "                            labelCol='digit (indexed)',\n",
    "                            maxDepth=5)\n",
    "print type(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we fit the estimator to get the **model**, which is a **transformer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_model = dt.fit(train)\n",
    "print type(dt_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_spark.ml_ does not support visualizations at the moment, but we do have a textual representation of the tree by calling the _toDebugString_ attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print dt_model.toDebugString[:500]  # partial printout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the features importances using the attribute [featureImportances][1], which returns a vector.\n",
    "\n",
    "[1]: http://spark.apache.org/docs/2.0.2/api/python/pyspark.ml.html#pyspark.ml.classification.DecisionTreeClassificationModel.featureImportances \"featureImportances API\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importances = dt_model.featureImportances\n",
    "zip('abcdefg', importances.toArray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply the model to predict the digit of the data used to train the model. We get three columns:\n",
    "\n",
    "* _rawPrediction_ - counts of the predictions assigned to this specific set of features\n",
    "* _probability_ - normalized rawPrediction, reflecting the probabilities of the predictions.\n",
    "* _prediction_ - the actual label corresponding to the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = dt_model.transform(train)\n",
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.select('rawPrediction').take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [MulticlassClassificationEvaluator][1] object to evaluat the model. This is a new type of object called **evaluator**.\n",
    "\n",
    "[1]: https://spark.apache.org/docs/2.0.2/api/python/pyspark.ml.html#pyspark.ml.evaluation.MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\",\n",
    "                                              labelCol=\"digit (indexed)\",\n",
    "                                              metricName=\"accuracy\")\n",
    "print type(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print evaluator.evaluate(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note about MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said before, the _pyspark.ml_ is constantly growing, but still it lacks many features that have already been developed and implemented in [_pyspark.mllib_][1]. One of these functionalities is the confusion matrix, and to illustrate teh differences we will now use the [_MulticlassMetrics_][2] method from the _pyspark.mllib.evaluation_ module.\n",
    "\n",
    "[1]: http://spark.apache.org/docs/2.0.2/api/python/pyspark.mllib.html \"pyspark.mllib API\"\n",
    "[2]: http://spark.apache.org/docs/2.0.2/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.MulticlassMetrics \"MulticlassMetrics API\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by noting that the _pyspark.mllib_ module utilizes **RDDs**, which are available thorugh the _rdd_ attribute of the _DataFrame_ class. From the documentation of the _MulticlassMetrics()_ method we learn that it requires an RDD containing only the predictions and the labels, so we select the relevant columns of the DataFrame using the method [_select()_][1] and create the proper RDD.\n",
    "\n",
    "[1]: http://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.select \"DataFrame.select() API\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictionAndLabels = train\\\n",
    "    .select(['prediction', 'digit (indexed)'])\\\n",
    "    .rdd\n",
    "predictionAndLabels.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "print type(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print metrics.confusionMatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** As part of the unified API pyspark is trying to expose, there is a class called [_Param_][1], which is utilized by all the models in _pyspark.ml_. In this session, however, we will use the parameters in their simplest form - key-word arguments of the estimator.\n",
    "\n",
    "[1]: http://spark.apache.org/docs/2.0.2/api/python/pyspark.ml.html#module-pyspark.ml.param \"pyspark.ml.param API\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each **estimator** contains some documentation about its parameters, available by the methods _explainParam()_ and _explainParams()_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print dt.explainParams()[:564]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print dt.explainParam('maxDepth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we instantiated the estimator we used the default values, but of course we can tweek the model by changing its parameters. The method [_setParams()_][1], which is available for all _pyspark.ml_ estimators, allows to easily modify the model parameters. We note that it is possible and advisable to use the [\\*\\*kwargs notation][2] when calling this method.\n",
    "\n",
    "[1]: http://spark.apache.org/docs/2.0.2/api/python/pyspark.ml.html#pyspark.ml.classification.DecisionTreeClassifier.setParams \"setParams() API\"\n",
    "[2]: http://book.pythontips.com/en/latest/args_and_kwargs.html \"kwargs explanation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'maxDepth':10}\n",
    "dt.setParams(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the parameters were indeed changed by looking at the explanation again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print dt.explainParam('maxDepth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-fitting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the steps of the process described earlier. We note that it is necessary to overwrite _train_ and _test_, otherwise we will get an error in the fit step, because a column with the name _prediction_ alredy exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = digits.randomSplit([0.7, 0.3], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = dt.fit(train).transform(train)\n",
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print evaluator.evaluate(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the model to the test data and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = dt_model.transform(test)\n",
    "test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print evaluator.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictionAndLabels = test\\\n",
    "    .select(['prediction', 'digit (indexed)'])\\\n",
    "    .rdd\n",
    "print MulticlassMetrics(predictionAndLabels).confusionMatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Your turn 1:**\n",
    "\n",
    "> * Part I - Create a classification model for predicting the sex of the guys listed in the weight.txt file based on their heights and weights.\n",
    "> * Part II - Repeat the previous task, but now use also the age as a feature. Did it improve the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Your turn 2:**\n",
    "\n",
    "> The file dessert.csv contains some information about groups who arrived at a restaurant, and the field “dessert” states whether they purchased a dessert or not. Use the file to develop a classification model for predicting which groups will order a dessert. Do not forget to split your data and validate your models.\n",
    "\n",
    "> * Part I - Develop the classification model, where “num.of.guests” is an integer.\n",
    "> * Part II - Improve your model by considering “num.of.guests” as categorical."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "name": "Decision trees with PySpark",
  "notebookId": 3948141445357954
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
