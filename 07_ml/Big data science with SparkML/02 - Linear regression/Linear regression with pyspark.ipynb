{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Linear regression with pyspark.ipynb","version":"0.3.2","provenance":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.11"},"kernelspec":{"display_name":"Python [default]","language":"python","name":"python2"}},"cells":[{"cell_type":"code","metadata":{"id":"UfU6c0U5rf3k","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":584},"outputId":"a339646c-a480-4472-92e2-500ddb38c1b9","executionInfo":{"status":"ok","timestamp":1563178273485,"user_tz":-180,"elapsed":41246,"user":{"displayName":"Naya College","photoUrl":"","userId":"00092338927245128733"}}},"source":["!apt-get update\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!wget -q http://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\n","!tar xf spark-2.3.1-bin-hadoop2.7.tgz\n","!pip install -q findspark\n","\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.1-bin-hadoop2.7\"\n","\n","import findspark\n","findspark.init()\n","from pyspark import SparkContext\n","sc = SparkContext.getOrCreate()\n","\n","import pyspark\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.getOrCreate()\n","\n","spark"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n","Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Hit:6 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Get:9 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n","Get:10 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Get:14 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [62.0 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [886 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [7,235 B]\n","Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,239 kB]\n","Get:18 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,663 kB]\n","Get:19 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [722 kB]\n","Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [578 kB]\n","Get:21 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [4,172 B]\n","Get:22 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [798 kB]\n","Fetched 6,232 kB in 6s (1,095 kB/s)\n","Reading package lists... Done\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://ab429a4997b5:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v2.3.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f776676a050>"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"6l5tDY0RrVMn","colab_type":"text"},"source":["# Linear regression with pyspark"]},{"cell_type":"code","metadata":{"id":"PVmdjQQwrVMo","colab_type":"code","colab":{}},"source":["from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n","from pyspark.ml.regression import LinearRegression\n","from pyspark.ml.evaluation import RegressionEvaluator"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RZKXjCNerVMs","colab_type":"text"},"source":["## Get the data"]},{"cell_type":"markdown","metadata":{"id":"Q9duiizbrVMt","colab_type":"text"},"source":["It is easier (in this pyspark version) to first load the data as an RDD, and then modify it into a dataFrame. During this process we also remove the header using a combination of _zipWithIndex()_ and _filter()_ (taken from [here][1]). By looking at the file we see the \"schema\", which is used by the second _map()_.\n","\n","[1]: http://stackoverflow.com/a/31798247/3121900"]},{"cell_type":"code","metadata":{"id":"cSnjQuKRrVMu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":337},"outputId":"467528f6-d4b8-4f7b-d891-cc847d01edc2","executionInfo":{"status":"error","timestamp":1563178280899,"user_tz":-180,"elapsed":1445,"user":{"displayName":"Naya College","photoUrl":"","userId":"00092338927245128733"}}},"source":["weights = spark.read.csv(\"/FileStore/tables/sl5xl2bb1490305146134/weight.txt\", \n","                         header=True, inferSchema=True)\n","weights.show(5)"],"execution_count":3,"outputs":[{"output_type":"error","ename":"AnalysisException","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mAnalysisException\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-3-af1f227d0068>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m weights = spark.read.csv(\"/FileStore/tables/sl5xl2bb1490305146134/weight.txt\", \n\u001b[0;32m----> 2\u001b[0;31m                          header=True, inferSchema=True)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.3.1-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.3.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.3.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAnalysisException\u001b[0m: u'Path does not exist: file:/FileStore/tables/sl5xl2bb1490305146134/weight.txt;'"]}]},{"cell_type":"markdown","metadata":{"id":"F0fuoCdnrVMx","colab_type":"text"},"source":["We already know that the age has no part in the model, so we drop the column."]},{"cell_type":"code","metadata":{"id":"3-ErAtrbrVMy","colab_type":"code","colab":{}},"source":["weights = weights.drop('Age')\n","weights.show(5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L_z1dhNDrVM1","colab_type":"text"},"source":["We will illustrate the basics with the boys data and then repeat the process for the girls."]},{"cell_type":"code","metadata":{"id":"tqGK4gWQrVM2","colab_type":"code","colab":{}},"source":["boys = weights.where(weights.Sex == 'm')\n","boys.show(5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aAwQPeKSrVM5","colab_type":"text"},"source":["### Vectorizing"]},{"cell_type":"markdown","metadata":{"id":"rnByS3FnrVM6","colab_type":"text"},"source":["While Spark DataFrames were designed to facilitate table-oriented tasks, they are not optimized for the mathematical manipulations required for applying the machine learnign algorithms. To overcome this problem, Spark offers another data structure called **Vector**, which is a list-like data structure.\n","\n","Its role will be more clear later, but for now we can think of it as a special column, collecting together several not-necessarily-the-same-type columns. Vectors can be created by constructors from the _pyspark.ml.linalg_ module, but they can also be created by assembling existing columns with the [_VectorAssembler_][va] transformer.\n","\n","[va]: http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler \"VectorAssembler() API\""]},{"cell_type":"code","metadata":{"id":"TMMcXWqYrVM7","colab_type":"code","colab":{}},"source":["va = VectorAssembler(inputCols=['Height'], outputCol='features')\n","print type(va)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RhFo6wPQrVM-","colab_type":"code","colab":{}},"source":["boys = va.transform(boys)\n","boys.show(5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sqHseY9frVND","colab_type":"text"},"source":["### Splitting the data"]},{"cell_type":"code","metadata":{"id":"RrbqcXcIrVNE","colab_type":"code","colab":{}},"source":["train_boys, test_boys = boys.randomSplit([0.7, 0.3], seed=1234)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RZnrtuw5rVNI","colab_type":"text"},"source":["## Single variable"]},{"cell_type":"markdown","metadata":{"id":"hKarLJAvrVNJ","colab_type":"text"},"source":["### Instantiate the model"]},{"cell_type":"markdown","metadata":{"id":"b2KQYuDhrVNK","colab_type":"text"},"source":["The model itself is embodied by the [LinearRegression][1] **estimator** class. The initialization of the estimator requires the declaration of the features by the argument _featuresCol_, the target by the argument _labelCol_ and the future prediction column by the argument _predictionCol_. It does **NOT** require the data itself...\n","\n","[1]: https://spark.apache.org/docs/2.0.2/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegression \"LinearRegression API\""]},{"cell_type":"code","metadata":{"collapsed":true,"id":"QdMypaKzrVNL","colab_type":"code","colab":{}},"source":["boys_lr = LinearRegression(featuresCol='features', \n","                           labelCol='Weight', \n","                           predictionCol='predicted weight')\n","print type(boys_lr)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_YzuDYfurVNQ","colab_type":"text"},"source":["### Fit the model"]},{"cell_type":"markdown","metadata":{"id":"U7-lChw8rVNR","colab_type":"text"},"source":["Being an **estimator**, a _LinearRegression_ object has a **_fit()_** method. This method applies the linear regression algorithm to fit the data in _featureCol_ to the labels in _labelCol_ to create a **model**, which is a type of **transformer**."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"5LvLdGvWrVNS","colab_type":"code","colab":{}},"source":["boys_lm = boys_lr.fit(train_boys)\n","print type(boys_lm)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RyU8V-GprVNY","colab_type":"text"},"source":["### Inspect the model"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"v6IbxV3TrVNZ","colab_type":"code","colab":{}},"source":["print boys_lm.coefficients\n","print boys_lm.intercept"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mNdKigjnrVNg","colab_type":"text"},"source":["Being a **transformer**, a _LinearRegressionModel_ object has a **_transform()_** method. This is the equivalent of the _predict()_ method from scikit-learn, and it applies the applies the model to the data and creates a new column with the name _predictionCol_."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"V_84nuu2rVNh","colab_type":"code","colab":{}},"source":["train_boys = boys_lm.transform(train_boys)\n","train_boys.show(5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mxQMtFYhrVNk","colab_type":"text"},"source":["### Assess the model"]},{"cell_type":"markdown","metadata":{"id":"57mLMJzhrVNl","colab_type":"text"},"source":["The RMSE (and other measures) are available in the _pyspark.ml.evaluation_ module. As usual, we instantiate an evaluator object with the proper arguments, and then apply it to the data."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"8mC9FewrrVNm","colab_type":"code","colab":{}},"source":["evaluator = RegressionEvaluator(predictionCol=\"predicted weight\", \n","                                labelCol=\"Weight\", \n","                                metricName=\"rmse\")\n","print evaluator.evaluate(train_boys)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gk_WYg4LrVNp","colab_type":"text"},"source":["### Validate the model"]},{"cell_type":"markdown","metadata":{"id":"EbM0yhxorVNq","colab_type":"text"},"source":["We apply the same steps to the test data ([pipeline][1], anyone?) and hope the results will be similar, otherwise we apparently have an overfitting problem.\n","\n","[1]: https://spark.apache.org/docs/2.0.2/ml-pipeline.html \"pipeline documentation\""]},{"cell_type":"code","metadata":{"collapsed":true,"id":"OjPsX-F8rVNs","colab_type":"code","colab":{}},"source":["test_boys = boys_lm.transform(test_boys)\n","print evaluator.evaluate(test_boys)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"95mc7xRfrVNz","colab_type":"text"},"source":["## Multiple variables"]},{"cell_type":"markdown","metadata":{"id":"d6s6WeqWrVN0","colab_type":"text"},"source":["The process is exactly the same, so we will show the entire code without verbal explanations and review it to note the minor differences."]},{"cell_type":"markdown","metadata":{"id":"oCOZPysbrVN1","colab_type":"text"},"source":["### Get the data"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"HvMy5_5hrVN3","colab_type":"code","colab":{}},"source":["diet = spark.read.csv(\"/FileStore/tables/gm30zbkj1490307234855/diet.txt\", \n","                      sep=';', header=True, inferSchema=True).drop('id')\n","\n","for col_name in diet.columns:\n","  diet = diet.withColumnRenamed(col_name, col_name.replace('.', '_'))\n","  \n","diet.show(5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y28RRjGmrVN6","colab_type":"text"},"source":["> **NOTE:** Spark does not allow features to have a dot (.) in their name."]},{"cell_type":"markdown","metadata":{"id":"t-2jTxZ7rVN7","colab_type":"text"},"source":["#### Vectorizing"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"7yOAnIgyrVOA","colab_type":"code","colab":{}},"source":["va = VectorAssembler(inputCols=diet.columns[:-1], outputCol='features')\n","diet = va.transform(diet)\n","diet.show(5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5m0abLQ2rVOD","colab_type":"text"},"source":["### Split the data"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"GJp70mkmrVOF","colab_type":"code","colab":{}},"source":["train_diet, test_diet = diet.randomSplit([0.7, 0.3], seed=1729)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l9auGwa_rVOK","colab_type":"text"},"source":["### Instantiate the model"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"1VV0VEr2rVOM","colab_type":"code","colab":{}},"source":["diet_lr = LinearRegression(featuresCol='features', \n","                           labelCol='change_kg', \n","                           predictionCol='predicted change')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YHbpRj4zrVOR","colab_type":"text"},"source":["### Fit the model"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"7wKT_VLarVOT","colab_type":"code","colab":{}},"source":["diet_lm = diet_lr.fit(train_diet)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cUcjLN4mrVOV","colab_type":"text"},"source":["### Inspect the model"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"5tiayG-WrVOW","colab_type":"code","colab":{}},"source":["print diet_lm.coefficients\n","print diet_lm.intercept"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4MLCs_m5rVOZ","colab_type":"text"},"source":["### Apply the model"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"9ygvB6p0rVOa","colab_type":"code","colab":{}},"source":["train_diet = diet_lm.transform(train_diet)\n","train_diet.show(5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oDRzo6RerVOe","colab_type":"text"},"source":["### Assess the model"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"RRzGj6dTrVOf","colab_type":"code","colab":{}},"source":["evaluator = RegressionEvaluator(predictionCol=\"predicted change\", \n","                                labelCol=\"change_kg\", \n","                                metricName=\"rmse\")\n","print evaluator.evaluate(train_diet)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7hIodHy7rVOi","colab_type":"text"},"source":["### Validate the model"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"8imRTjdBrVOj","colab_type":"code","colab":{}},"source":["test_diet = diet_lm.transform(test_diet)\n","print evaluator.evaluate(test_diet)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c0Vr_1QhrVOm","colab_type":"text"},"source":["> **Your turn 1:** Read the grades.txt file. For the sake of this exercise you may ignore the splitting step and use the entire data for the regression.\n","\n","> * Part I - Fit three single-variable regression models for the SAT grade based on each of the math grade, the english grade and the literature grade, and analyze them. Which of the models is the best?\n","> * Part II - Fit a new linear regression model with all three grades as predictors, and analyze the model. Is the new model better than the previous ones?"]},{"cell_type":"markdown","metadata":{"id":"MM9i7XuarVOn","colab_type":"text"},"source":["## Dummy variables"]},{"cell_type":"markdown","metadata":{"id":"Q2HFmjPxrVOo","colab_type":"text"},"source":["The concept of dummy variables is implemented in _pyspark.ml_ by a combination of two optional **estimators and transformers** - [_StringIndexer_][1] and [_OneHotEncoder_][2]. _StringIndexer_ maps a \"categorical\" feature column of type string into arbitrary integers, and _OneHotEncoder_ maps a column of category indices to a column of binary vectors, with at most a single one-value per row that indicates the input category index. It sounds complicated, but it is not...\n","\n","More generally, the module [_features_][3] of _pyspark.ml_ supports a large family of data transformers, which are documented [here][4].\n","\n","[1]: https://spark.apache.org/docs/2.0.2/api/python/pyspark.ml.html#pyspark.ml.feature.StringIndexer \"StringIndexer API\"\n","[2]: https://spark.apache.org/docs/2.0.2/api/python/pyspark.ml.html#pyspark.ml.feature.OneHotEncoder \"OneHotEncoder API\"\n","[3]: https://spark.apache.org/docs/2.0.2/api/python/pyspark.ml.html#module-pyspark.ml.feature \"ml.features API\"\n","[4]: https://spark.apache.org/docs/2.0.2/ml-features.html \"ml.features documentation\""]},{"cell_type":"markdown","metadata":{"id":"7pXZDuR5rVOp","colab_type":"text"},"source":["To apply this concept to the _gender_ feature we roll back to the step before the vectorizing. This time we consider both boys and girls."]},{"cell_type":"markdown","metadata":{"id":"hXBcLrtQrVOp","colab_type":"text"},"source":["#### Indexing"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"iterLW6arVOu","colab_type":"code","colab":{}},"source":["si = StringIndexer(inputCol='Sex', outputCol='Sex (indexed)')\n","si_model = si.fit(weights)\n","weights = si_model.transform(weights)\n","weights.show(5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3qP0yHYkrVOy","colab_type":"text"},"source":["#### Encoding"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"TvLSwPhFrVOz","colab_type":"code","colab":{}},"source":["ohe = OneHotEncoder(inputCol='Sex (indexed)', outputCol='Sex (one hot)')\n","ohe.setDropLast(False)\n","weights = ohe.transform(weights)\n","weights.show(5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dsn97KC9rVO1","colab_type":"text"},"source":["> **NOTE:** _OneHotEncoder()_ returns [sparse vectors][1], which is a standard representation of arrays with a lot of zeroes. In this representation, the tuple (_n_, [_locs_], [_vals_]) means there are _n_ elements in the vector, and the value in location _locs[i]_ is _vals[i]_. This makes the illustration not very intuitive, but we will have to deal with that...\n","\n","[1]: https://en.wikipedia.org/wiki/Sparse_array \"Sparse array - Wikipedia\""]},{"cell_type":"markdown","metadata":{"id":"bLGhHolurVO2","colab_type":"text"},"source":["#### Vectorizing"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"x1nicCfurVO3","colab_type":"code","colab":{}},"source":["va = VectorAssembler(inputCols=['Height', 'Sex (one hot)'], outputCol='features')\n","weights = va.transform(weights)\n","weights.show(5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4x_5EMw0rVO5","colab_type":"text"},"source":["### Split the data"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"I7FJAO7UrVO6","colab_type":"code","colab":{}},"source":["train_weights, test_weights = weights.randomSplit([0.7, 0.3], seed=8128)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WlswXVO6rVO8","colab_type":"text"},"source":["### Instantiate the model"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"TgVBQZ-0rVO9","colab_type":"code","colab":{}},"source":["weight_lr = LinearRegression(featuresCol='features', \n","                             labelCol='Weight', \n","                             predictionCol='predicted weight',\n","                             solver='bfgs')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MdFC2g96rVPD","colab_type":"text"},"source":["> **NOTE:** For a reason not clear to me thee default linear regression does not work properly, and I had to specifically state the BFGS algorithm."]},{"cell_type":"markdown","metadata":{"id":"kQx5v3CGrVPE","colab_type":"text"},"source":["### Fit the model"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"5HhHhxbirVPG","colab_type":"code","colab":{}},"source":["weight_lm = weight_lr.fit(weights)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_GVLdgB1rVPL","colab_type":"text"},"source":["### Inspect the model"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"YZa-FmI5rVPM","colab_type":"code","colab":{}},"source":["print weight_lm.coefficients\n","print weight_lm.intercept"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-2VLsiSWrVPP","colab_type":"text"},"source":["### Apply the model"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"bi4q86wvrVPP","colab_type":"code","colab":{}},"source":["train_weights = weight_lm.transform(train_weights)\n","train_weights.show(5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WB_GBRb4rVPU","colab_type":"text"},"source":["### Assess the model"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"sfnCY2w6rVPV","colab_type":"code","colab":{}},"source":["evaluator = RegressionEvaluator(predictionCol=\"predicted weight\", \n","                                labelCol=\"Weight\", \n","                                metricName=\"rmse\")\n","print evaluator.evaluate(train_weights)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xQ9OiEPurVPY","colab_type":"text"},"source":["### Validate the model"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"rUU822oHrVPY","colab_type":"code","colab":{}},"source":["test_weights = weight_lm.transform(test_weights)\n","print evaluator.evaluate(test_weights)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6HODWLIMrVPd","colab_type":"text"},"source":["> **Your turn 2:** The file prices.csv contains rental details for many apartments in several cities. Read the file, use its data to create two linear models for estimating the price (part I and part II below), and explain which one is better and why.\n","\n","> * Part I - The ‘Rooms’ feature is an integer.\n","> * Part II - The ‘Rooms’ feature is a dummy variables."]}]}