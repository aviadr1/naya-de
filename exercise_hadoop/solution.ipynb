{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "In this exercise we will upload the audiostore database from MySQL to Impala. We will do it in the following steps:\n",
    "\n",
    "1. Locate/create the database in MySQL\n",
    "2. Create/clean the staging folder in HDFS\n",
    "3. Make the list of tables in the database\n",
    "4. Upload the tables to the staging folder as Parquet files\n",
    "5. Load the data into Impala\n",
    "6. Query your data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "> Make sure you have the audiostore database in MySQL\n",
    "\n",
    "use `import os` and `os.system` to run cli commands to create a database and run the audiostoreDB.sql script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blah\n",
      "mysql: [Warning] Using a password on the command line interface can be insecure.\n",
      "+--------------------+\n",
      "| Database           |\n",
      "+--------------------+\n",
      "| information_schema |\n",
      "| amon               |\n",
      "| audiostore         |\n",
      "| classicmodels      |\n",
      "| hue                |\n",
      "| metastore          |\n",
      "| mysql              |\n",
      "| nav                |\n",
      "| navms              |\n",
      "| oozie              |\n",
      "| performance_schema |\n",
      "| rman               |\n",
      "| scm                |\n",
      "| sentry             |\n",
      "| sys                |\n",
      "+--------------------+\n",
      "mysql: [Warning] Using a password on the command line interface can be insecure.\n",
      "mysql: [Warning] Using a password on the command line interface can be insecure.\n",
      "mysql: [Warning] Using a password on the command line interface can be insecure.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# os.system('echo blah')\n",
    "!echo blah\n",
    "\n",
    "# os.system(\"mysql -unaya -pNayaPass1!  -e 'show databases;' \")\n",
    "!mysql -unaya -pNayaPass1!  -e 'show databases;' \n",
    "\n",
    "# # drop database if exists\n",
    "# os.system(\"sudo mysql -unaya -pNayaPass1!  -e 'DROP DATABASE IF EXISTS audiostore;' \")\n",
    "!sudo mysql -unaya -pNayaPass1!  -e 'DROP DATABASE IF EXISTS audiostore;'\n",
    "\n",
    "# # create database audiostore\n",
    "# os.system(\"sudo mysql -unaya -pNayaPass1!  -e 'create database audiostore;' \")\n",
    "!sudo mysql -unaya -pNayaPass1!  -e 'create database audiostore;'\n",
    "\n",
    "# run databases.sql\n",
    "# os.system(\"sudo mysql -unaya -pNayaPass1!  audiostore<AudioStoreDB.sql \")\n",
    "!sudo mysql -unaya -pNayaPass1!  audiostore<AudioStoreDB.sql\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 2 \n",
    "We are going to use the temporary folder /tmp/staging/ for storing the tables before we upload them into\n",
    "HDFS. Make sure this folder exists (if not - create it) and that it is empty.\n",
    "\n",
    "\n",
    "- Use pyarrow to create a HadoopFileSystem instance (usually called fs) to control HDFS functionalities.\n",
    "- Use simple Python scripting with the exists(), mkdir() and delete() methods.\n",
    "\n",
    "dont forget `import pyarrow as pa` or altenatively, use the file `hadoop_config.py` where we have all the definitions by using `import hadoop_config` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyarrow as pa\n",
    "from hadoop_config import fs, cnx as mysql_cnx, hdfs, client as impala_client, hive_cnx\n",
    "\n",
    "# fs = pa.hdfs.HadoopFileSystem(\n",
    "#     host='Cnt7-naya-cdh63', #or internal-ip\n",
    "#     port=8020,\n",
    "#     user='hdfs',\n",
    "#     kerb_ticket=None,\n",
    "#     extra_conf=None)\n",
    "\n",
    "if fs.exists('/tmp/staging'):\n",
    "    fs.delete('/tmp/staging', recursive=True)\n",
    "fs.mkdir('/tmp/staging', recursive=True)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3\n",
    "In this  step we would like to iterate the tables in\n",
    "order to upload them as Parquet files, so in this step you should get the list of tables in the database.\n",
    "\n",
    "- Use any RDBMS API to connect to the database. SQLAlchemy is a good option, but consider using a MySQL connector (like the official mysql-connector-python, which is already installed (explained better at W3Schools).\n",
    "- Execute an SQL query to get all the names of the tables in the database, and then use the fetchall() method to turn the result into a list.\n",
    "\n",
    "remember the file `hadoop_config.py` already contains all the definitions we need so do `from hadoop_config import cnx as mysql_cnx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             album\n",
       "1            artist\n",
       "2          customer\n",
       "3          employee\n",
       "4             genre\n",
       "5           invoice\n",
       "6       invoiceline\n",
       "7         mediatype\n",
       "8          playlist\n",
       "9     playlisttrack\n",
       "10            track\n",
       "Name: Tables_in_audiostore, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tables = pd.read_sql(\"SHOW TABLES\", mysql_cnx)\n",
    "tables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4\n",
    "\n",
    "Import all the tables from the audiostore database into the staging folder using the Parquet format.\n",
    "\n",
    "\n",
    "- There is going to be a for-loop on the list of tables. Each table will be converted to a Parquet file, which will be uploaded to our HDFS staging area.\n",
    "- I couldn't find a direct option to make this transition, so I've added an intermediate staging step through pandas.DataFrame. Since pandas is so popular, it is easier to first read the tables as pandas.DataFrame (using read_sql()) and then write them as Parquet (using write_table()).\n",
    "- Parquet does not support complex datetime types, so you might have to manipulate your data so that Parquet will \"eat it\".\n",
    "- Parquet files have no standard encoding, so you should consider them as a stream of binary files.\n",
    "\n",
    "these imports are useful\n",
    "```\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "album\n",
      "chunk 0\n",
      "artist\n",
      "chunk 0\n",
      "customer\n",
      "chunk 0\n",
      "employee\n",
      "chunk 0\n",
      "genre\n",
      "chunk 0\n",
      "invoice\n",
      "chunk 0\n",
      "invoiceline\n",
      "chunk 0\n",
      "mediatype\n",
      "chunk 0\n",
      "playlist\n",
      "chunk 0\n",
      "playlisttrack\n",
      "chunk 0\n",
      "track\n",
      "chunk 0\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "batch_size = 10000\n",
    "for table in tables.squeeze().to_list():\n",
    "    print(table)\n",
    "    path_table = '/tmp/staging/' + table + '.parquet'\n",
    "    with fs.open(path_table, \"wb\") as fw:\n",
    "        for i, chunk in enumerate(pd.read_sql(f\"SELECT * FROM {table}\", mysql_cnx, chunksize=batch_size)):\n",
    "            print(f\"chunk {i}\")\n",
    "            df_for_hdfs = pa.Table.from_pandas(chunk)\n",
    "            pq.write_table(df_for_hdfs, fw)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Table.from_arrays(arrays, names=None, schema=None, dict metadata=None)\n",
      "\n",
      "Construct a Table from Arrow arrays or columns\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "arrays: list of pyarrow.Array or pyarrow.Column\n",
      "    Equal-length arrays that should form the table.\n",
      "names: list of str, optional\n",
      "    Names for the table columns. If Columns passed, will be\n",
      "    inferred. If Arrays passed, this argument is required\n",
      "\n",
      "Returns\n",
      "-------\n",
      "pyarrow.Table\n",
      "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
     ]
    }
   ],
   "source": [
    "pa.Table.from_arrays?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
