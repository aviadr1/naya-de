{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/java/jdk1.8.0_181-cloudera\n"
     ]
    }
   ],
   "source": [
    "!echo $JAVA_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = pa.hdfs.HadoopFileSystem(\n",
    "    host='cnt7-naya-cdh63',\n",
    "    port=8020,\n",
    "    user='hdfs',\n",
    "    kerb_ticket=None,\n",
    "    driver='libhdfs',\n",
    "    extra_conf=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96414515200\n",
      "91948.0 MB\n"
     ]
    }
   ],
   "source": [
    "cap = fs.get_capacity()\n",
    "\n",
    "print(cap)\n",
    "print(str(round(cap / pow(1024,2), 0)) + ' MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/tmp/hive/hive/19acffe3-acc4-43ab-ad74-b355ee5868a8', '/tmp/hive/hive/2578e566-5503-47ca-bb44-16b1d3df0270', '/tmp/hive/hive/338a963d-da54-4df1-93f7-482007d33f72', '/tmp/hive/hive/3aee8966-4e93-499e-8f04-17c7ca903763', '/tmp/hive/hive/6915a150-1168-4785-858a-36fec5fff258', '/tmp/hive/hive/6bfb7742-65d3-4b41-9b20-671f751ec554', '/tmp/hive/hive/76eda73c-22e3-4f8d-9285-c7dc5e7e80dc', '/tmp/hive/hive/a6062efd-ea92-487e-89b0-8c41f19f376a', '/tmp/hive/hive/c324e2de-e526-4a93-ae63-0014d83570a1', '/tmp/hive/hive/fbf7b2d6-179d-4f04-890c-6e81c8c9cd57']\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# folder = 'hdfs://Cnt7-naya-cdh63:8020/tmp/sqoop/staging'\n",
    "# try:\n",
    "#     # Del if exist /tmp/sqoop/staging.\n",
    "#     fs.rm(folder, recursive=True)\n",
    "# except pa.ArrowIOError:\n",
    "#     if not fs.isdir(folder):\n",
    "#         raise\n",
    "\n",
    "\n",
    "# Finally, using ls() we verify the content of the folder\n",
    "sql_dir_files = fs.ls('/tmp/hive/hive', detail=False)\n",
    "print(sql_dir_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['classicmodels.sql', 'ordsTable.sql']\n",
      "['/tmp/sqoop/staging/classicmodels.sql', '/tmp/sqoop/staging/ordsTable.sql']\n",
      "Let's transfer files from hdfs to local OS:\n",
      "classicmodels.sql\n"
     ]
    }
   ],
   "source": [
    "### EXAMPLE2.py\n",
    "\n",
    "'''\n",
    "upload files created locally to the HDFS staging area.\n",
    "download data from HDFS to our local system\n",
    "'''\n",
    "import os\n",
    "import pyarrow as pa\n",
    "\n",
    "fs = pa.hdfs.HadoopFileSystem(\n",
    "    host='Cnt7-naya-cdh63',\n",
    "    port=8020,\n",
    "    user='hdfs',\n",
    "    kerb_ticket=None,\n",
    "    extra_conf=None)\n",
    "\n",
    "#Note: will delete only if exist /tmp/sqoop/staging\n",
    "if fs.exists('hdfs://Cnt7-naya-cdh63:8020/tmp/sqoop/staging'):\n",
    "    fs.rm('hdfs://Cnt7-naya-cdh63:8020/tmp/sqoop/staging', recursive=True)\n",
    "\n",
    "\n",
    "#First we use mkdir() to create a staging area in HDFS under /tmp/sqoop/staging.\n",
    "fs.mkdir('hdfs://Cnt7-naya-cdh63:8020/tmp/sqoop/staging', create_parents=True)\n",
    "\n",
    "#Next, we use upload() to copy all *.sql files created locally to the staging area.\n",
    "\n",
    "local_path = '/home/naya/tutorial/sqoop/scripts/'\n",
    "extension = '.sql'\n",
    "sql_files = [f for f in os.listdir(local_path) if f.endswith(extension)]\n",
    "print(sql_files)\n",
    "\n",
    "#,'rb'\n",
    "for f_name in sql_files:\n",
    "    with open(local_path + f_name,'rb') as f:\n",
    "        fs.upload('hdfs://Cnt7-naya-cdh63:8020/tmp/sqoop/staging/{}'.format(f_name), f)\n",
    "\n",
    "# Finally, using ls() we verify the content of the folder\n",
    "sql_dir_files = fs.ls('hdfs://Cnt7-naya-cdh63:8020/tmp/sqoop/staging', detail=False)\n",
    "print(sql_dir_files)\n",
    "\n",
    "#We can also download data from HDFS to our local system using the download() method.\n",
    "print(\"Let's transfer files from hdfs to local OS:\")\n",
    "fs.download(path='hdfs://Cnt7-naya-cdh63:8020/tmp/sqoop/staging/classicmodels.sql',\n",
    "            stream='/home/naya/tutorial/sqoop/classicmodels.sql',\n",
    "            buffer_size=None)\n",
    "\n",
    "# Now we can check if the file was transfer to a local directory\n",
    "local_files = [f for f in os.listdir('/home/naya/tutorial/sqoop') if f.endswith(extension)]\n",
    "for f_name in local_files:\n",
    "    print(f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path': 'hdfs://Cnt7-naya-cdh63:8020/tmp/sqoop/staging/classicmodels.sql', 'owner': 'naya', 'group': 'naya', 'size': 202062, 'block_size': 134217728, 'last_modified': 1678656508, 'last_accessed': 1678656507, 'replication': 1, 'permissions': 775, 'kind': 'file'}\n",
      "kind : file\n",
      "name : /tmp/sqoop/staging/classicmodels.sql\n",
      "owner : naya\n",
      "group : naya\n",
      "last_modified_time : 1678656508\n",
      "last_access_time : 1678656507\n",
      "size : 202062\n",
      "replication : 1\n",
      "block_size : 134217728\n",
      "permissions : 775\n"
     ]
    }
   ],
   "source": [
    "### example3.py\n",
    "\n",
    "'''\n",
    "Managing permissions with chmod() & chown()\n",
    "In this example we manage permissions for our files\n",
    "'''\n",
    "\n",
    "import os\n",
    "import pyarrow as pa\n",
    "\n",
    "fs = pa.hdfs.HadoopFileSystem(\n",
    "    host='Cnt7-naya-cdh63',\n",
    "    port=8020,\n",
    "    user='hdfs',\n",
    "    kerb_ticket=None,\n",
    "    extra_conf=None)\n",
    "\n",
    "# Change access permissions for the file classicmodels.sql in the staging directory.\n",
    "fs.chmod('hdfs://Cnt7-naya-cdh63:8020/tmp/sqoop/staging/classicmodels.sql', 775)\n",
    "\n",
    "# Change access permissions for a file in the staging directory in order\n",
    "# to allow for naya user to get this file only\n",
    "fs.chown('hdfs://Cnt7-naya-cdh63:8020/tmp/sqoop/staging/classicmodels.sql', owner='naya', group='naya')\n",
    "\n",
    "# Finally, we can chaeck the details of the files either by the info() method.\n",
    "a=fs.info('hdfs://Cnt7-naya-cdh63:8020/tmp/sqoop/staging/classicmodels.sql')\n",
    "print(a)\n",
    "\n",
    "# Alternatively we can do it using the detail=True argument of the ls() method.\n",
    "file_details = fs.ls('hdfs://Cnt7-naya-cdh63:8020/tmp/sqoop/staging/classicmodels.sql', detail=True)\n",
    "for detail in file_details:\n",
    "    for k, v in detail.items():\n",
    "        print(k, ':', v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "Get the amount of used space on your file system.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "643.0 MB\n",
      "674713158\n",
      "91305.0 MB\n",
      "95739802042\n"
     ]
    }
   ],
   "source": [
    "fs = pa.hdfs.HadoopFileSystem(\n",
    "    host='Cnt7-naya-cdh63',\n",
    "    port=8020,\n",
    "    user='hdfs',\n",
    "    kerb_ticket=None,\n",
    "    extra_conf=None)\n",
    "\n",
    "used_space = fs.disk_usage('hdfs://Cnt7-naya-cdh63:8020/')\n",
    "print(str(round(used_space / pow(1024,2), 0)) + ' MB')\n",
    "print(used_space)\n",
    "\n",
    "# 2. Get reported free space of the file system in bytes and modify this output into a human readable format.\n",
    "path='hdfs://Cnt7-naya-cdh63:8020/'\n",
    "cap=fs.get_capacity()\n",
    "disk_usage=fs.disk_usage(path)\n",
    "free_space=cap-disk_usage\n",
    "\n",
    "print(str(round(free_space / pow(1024, 2), 0)) + ' MB')\n",
    "print(free_space )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "\n",
    "Use the delete method to delete some of the files from the staging directory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['classicmodels.sql', 'ordsTable.sql']\n",
      "['/tmp/sqoop/staging']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def create_files_example_2():\n",
    "    #First we use mkdir() to create a staging area in HDFS under /tmp/sqoop/staging.\n",
    "    fs.mkdir('hdfs://Cnt7-naya-cdh63:8020/tmp/sqoop/staging', create_parents=True)\n",
    "\n",
    "    #Next, we use upload() to copy all *.sql files created locally to the staging area.\n",
    "\n",
    "    local_path = '/home/naya/tutorial/sqoop/scripts/'\n",
    "    extension = '.sql'\n",
    "    sql_files = [f for f in os.listdir(local_path) if f.endswith(extension)]\n",
    "    print(sql_files)\n",
    "\n",
    "    #,'rb'\n",
    "    for f_name in sql_files:\n",
    "        with open(local_path + f_name,'rb') as f:\n",
    "            fs.upload('hdfs://Cnt7-naya-cdh63:8020/tmp/sqoop/staging/{}'.format(f_name), f)\n",
    "\n",
    "create_files_example_2()\n",
    "# using ls() we verify the content of the folder\n",
    "sql_dir_files = fs.ls('hdfs://Cnt7-naya-cdh63:8020/tmp/sqoop', detail=False)\n",
    "print(sql_dir_files)\n",
    "\n",
    "fs.rm('hdfs://Cnt7-naya-cdh63:8020/tmp/sqoop/staging', recursive=True)\n",
    "\n",
    "\n",
    "# Finally, using ls() we verify the content of the folder\n",
    "sql_dir_files = fs.ls('hdfs://Cnt7-naya-cdh63:8020/tmp/sqoop', detail=False)\n",
    "print(sql_dir_files)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "1. Connect to a local HDFS with parameters\n",
    "2. Get reported free space of the file system in bytes and modify this output into a human readable format\n",
    "3. Get disk usage information about /user directory in HDFS in bytes and modify this output into a human readable format\n",
    "4. Get detailed information about a HDFS directory /user/hive/warehouse in human readable format\n",
    "5. Create a directory in HDFS: first check if a directory exists and through the message about it, if a directory don't\n",
    " exist create it. Use a global scope's variable. Check the script with an existing path too.\n",
    "6. Rename a file in HDFS and check if the file was renamed.\n",
    "7. Delete the whole directory /tmp/sqoop/staging and it's files and check directory again.\n",
    "8. Create a new file in any directory on your OS and write any row into it (hint: use input() method). Upload this file\n",
    "to HDFS directory.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "space 643MB / 91948MB\n",
      "used by hdfs:///user: 641MB\n",
      "/user/hive/warehouse/classicmodels.db\n",
      "\t kind : directory\n",
      "\t name : /user/hive/warehouse/classicmodels.db\n",
      "\t owner : hdfs\n",
      "\t group : hive\n",
      "\t last_modified_time : 1678656018\n",
      "\t last_access_time : 0\n",
      "\t size : 0\n",
      "\t replication : 0\n",
      "\t block_size : 0\n",
      "\t permissions : 1023\n",
      "creating hdfs://Cnt7-naya-cdh63:8020/tmp/sqoop/staging/\n",
      "creating hdfs://Cnt7-naya-cdh63:8020/tmp/sqoop/staging/stam.txt\n",
      "['/tmp/sqoop/staging/stam.txt']\n",
      "hello hdfs!\n",
      "\n",
      "directory exists now: True\n",
      "deleting hdfs://Cnt7-naya-cdh63:8020/tmp/sqoop/staging/\n",
      "directory exists after deleting: False\n",
      "we can upload!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Connect to a local HDFS with parameters such: host, port, user, kerberos ticket for authentication if exists,\n",
    "# driver and other configurations.\n",
    "fs = pa.hdfs.HadoopFileSystem (\n",
    "    host='Cnt7-naya-cdh63',\n",
    "    port=8020,\n",
    "    user='hdfs',\n",
    "    kerb_ticket=None,\n",
    "    extra_conf=None)\n",
    "\n",
    "# 2. Get reported free space of the file system in bytes and modify this output into a human readable format.\n",
    "used_space = pa.hdfs.HadoopFileSystem.disk_usage(fs,'hdfs://Cnt7-naya-cdh63:8020/')\n",
    "cap=pa.hdfs.HadoopFileSystem.get_capacity(fs)\n",
    "MB = 1024 ** 2\n",
    "print(f\"space {used_space//MB}MB / {cap//MB}MB\")\n",
    "\n",
    "# 3. Get disk usage information about /user directory in HDFS in bytes and modify this output\n",
    "# into a human readable format.\n",
    "used_space = pa.hdfs.HadoopFileSystem.disk_usage(fs,'/user')\n",
    "print(f\"used by hdfs:///user: {used_space//MB}MB\")\n",
    "\n",
    "# 4. Get detailed information about a HDFS directory /user/hive/warehouse in human readable format.\n",
    "files = fs.ls('/user/hive/warehouse', detail=True)\n",
    "for file in files:\n",
    "    print(file['name'])\n",
    "    for key,value in file.items():\n",
    "        print('\\t', key, ':', value)\n",
    "\n",
    "# 5. Create a directory in HDFS: first check if a directory exists and through the message about it,\n",
    "# if a directory don't exist create it.\n",
    "# Use a global scope's variable. Check the script with an existing path too.\n",
    "p = 'hdfs://Cnt7-naya-cdh63:8020/tmp/sqoop/staging/'\n",
    "if fs.exists(p):\n",
    "    print(p, 'already exists')\n",
    "else:\n",
    "    print('creating', p)\n",
    "    fs.mkdir(p, create_parents=True)\n",
    "\n",
    "# 6. Rename a file in HDFS and check if the file was renamed.\n",
    "src = p+'stam.txt'\n",
    "print('creating', src)\n",
    "with fs.open(src, 'wb') as f:\n",
    "    f.write('hello hdfs!\\n'.encode())\n",
    "    f.close()\n",
    "\n",
    "print(fs.ls(p))\n",
    "dst = p + 'hello.txt'\n",
    "if fs.exists(dst):\n",
    "    fs.rm(dst)\n",
    "fs.rename(src, dst)\n",
    "\n",
    "print(fs.cat(p+'hello.txt').decode())\n",
    "\n",
    "# 7. Delete the whole directory /tmp/sqoop/staging and it's files and check directory again.\n",
    "print('directory exists now:', fs.exists(p))\n",
    "print('deleting', p)\n",
    "fs.rm(p, recursive=True)\n",
    "print('directory exists after deleting:', fs.exists(p))\n",
    "\n",
    "# 8. Create a new file in any directory on your OS and write any row into it (hint: use input() method).\n",
    "#    Upload this file to HDFS directory.\n",
    "p = '/home/naya/tutorial/sqoop/scripts/file.txt'\n",
    "with open(p, 'w') as f:\n",
    "    print('we can upload!', file=f)\n",
    "with open(p,'rb') as f:\n",
    "    fs.upload('hdfs://Cnt7-naya-cdh63:8020/tmp/sqoop/development/file.txt', f)\n",
    "\n",
    "print(fs.cat('hdfs://Cnt7-naya-cdh63:8020/tmp/sqoop/development/file.txt').decode())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
